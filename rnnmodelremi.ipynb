{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59576a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/music-x-lab/POP909-Dataset.git\n",
    "!cd POP909-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c250f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schng\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from symusic import Score\n",
    "from miditok import REMI\n",
    "import os\n",
    "\n",
    "tokenizer = REMI()\n",
    "\n",
    "def extract_tokens_from_midi(file_path: str):\n",
    "    score = Score.from_file(file_path)\n",
    "\n",
    "    token_sequences = tokenizer(score)\n",
    "\n",
    "    if not token_sequences:\n",
    "        return []\n",
    "\n",
    "    return token_sequences[0].tokens  # first track's tokens\n",
    "\n",
    "\n",
    "def collect_pop909_tokens(base_dir: str = r\"C:\\\\Documents\\\\CompSci\\\\CSE153\\\\assignment2\\\\POP909-Dataset\\\\POP909\"):\n",
    "    \"\"\"Walk the POP909 folder structure and return a flat list of REMI tokens for all MIDI files.\n",
    "\n",
    "    Args:\n",
    "        base_dir: Root directory of the POP909 dataset.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A flat list of all REMI tokens from the dataset.\n",
    "    \"\"\"\n",
    "    all_tokens: list[str] = []\n",
    "\n",
    "    for folder_id in range(1, 910):\n",
    "        folder_name = f\"{folder_id:03d}\"\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(folder_path):\n",
    "            if fname.lower().endswith((\".mid\", \".midi\")):\n",
    "                fpath = os.path.join(folder_path, fname)\n",
    "                try:\n",
    "                    tokens = extract_tokens_from_midi(fpath)\n",
    "                    all_tokens.extend(tokens)  # append tokens to global list\n",
    "                except Exception as exc:\n",
    "                    print(f\"[WARN] Failed to tokenize {fpath}: {exc}\")\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6772236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bar_None', 'Bar_None', 'Bar_None', 'Bar_None', 'Bar_None', 'Position_25', 'Pitch_61', 'Velocity_115', 'Duration_0.1.8', 'Position_27', 'Pitch_63', 'Velocity_111', 'Duration_0.1.8', 'Position_29', 'Pitch_66', 'Velocity_119', 'Duration_0.1.8', 'Position_31', 'Pitch_68', 'Velocity_111', 'Duration_0.1.8', 'Bar_None', 'Position_1', 'Pitch_70', 'Velocity_111', 'Duration_0.1.8', 'Position_5', 'Pitch_66', 'Velocity_111', 'Duration_0.2.8', 'Position_9', 'Pitch_63', 'Velocity_119', 'Duration_0.3.8', 'Position_13', 'Pitch_68', 'Velocity_115', 'Duration_1.4.8', 'Bar_None', 'Position_1', 'Pitch_68', 'Velocity_111', 'Duration_0.3.8', 'Position_5', 'Pitch_65', 'Velocity_115', 'Duration_0.2.8', 'Position_9', 'Pitch_61', 'Velocity_115', 'Duration_0.2.8', 'Position_13', 'Pitch_66', 'Velocity_119', 'Duration_1.1.8', 'Position_25', 'Pitch_61', 'Velocity_111', 'Duration_0.1.8', 'Position_27', 'Pitch_63', 'Velocity_111', 'Duration_0.1.8', 'Position_29', 'Pitch_66', 'Velocity_115', 'Duration_0.1.8', 'Position_31', 'Pitch_68', 'Velocity_111', 'Duration_0.1.8', 'Bar_None', 'Position_1', 'Pitch_70', 'Velocity_111', 'Duration_0.1.8', 'Position_5', 'Pitch_66', 'Velocity_111', 'Duration_0.3.8', 'Position_9', 'Pitch_63', 'Velocity_115', 'Duration_0.2.8', 'Position_13', 'Pitch_68', 'Velocity_115', 'Duration_0.6.8', 'Position_21', 'Pitch_61', 'Velocity_115', 'Duration_0.2.8', 'Position_25', 'Pitch_68', 'Velocity_115', 'Duration_0.2.8', 'Position_29', 'Pitch_66', 'Velocity_119', 'Duration_1.7.8']\n"
     ]
    }
   ],
   "source": [
    "token_sequence = collect_pop909_tokens()\n",
    "print(token_sequence[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\schng\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m20435/20435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 19ms/step - loss: 2.2089\n",
      "Epoch 2/5\n",
      "\u001b[1m14729/20435\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 20ms/step - loss: 1.7568"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# 1. Build vocab\n",
    "unique_tokens = sorted(set(token_sequence))\n",
    "token_to_id = {tok: i for i, tok in enumerate(unique_tokens)}\n",
    "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "# 2. Encode tokens to ids\n",
    "encoded_sequence = [token_to_id[tok] for tok in token_sequence]\n",
    "\n",
    "seq_length = 20  # how many tokens in input sequence\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(encoded_sequence) - seq_length):\n",
    "    inputs.append(encoded_sequence[i:i+seq_length])\n",
    "    targets.append(encoded_sequence[i+1:i+seq_length+1])\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)\n",
    "\n",
    "vocab_size = len(unique_tokens)\n",
    "embedding_dim = 64\n",
    "rnn_units = 128\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "    layers.LSTM(rnn_units, return_sequences=True),\n",
    "    layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(inputs, targets, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, seed_sequence, gen_length=1000, temperature=1.0):\n",
    "    generated = list(seed_sequence)\n",
    "    for _ in range(gen_length):\n",
    "        input_seq = np.array(generated[-seq_length:])[None, :]  # batch size 1\n",
    "        preds = model.predict(input_seq)[0, -1]\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        next_id = np.random.choice(len(preds), p=preds)\n",
    "        generated.append(next_id)\n",
    "    return generated\n",
    "\n",
    "# Start generation with the first sequence as seed\n",
    "seed_seq = encoded_sequence[:seq_length]\n",
    "generated_ids = generate_tokens(model, seed_seq)\n",
    "\n",
    "generated_tokens = [id_to_token[i] for i in generated_ids]\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_tokens)\n",
    "\n",
    "score = tokenizer([generated_tokens])  # this is equivalent to tokenizer.decode()\n",
    "\n",
    "# Save to MIDI\n",
    "score.dump_midi(\"generated_piece.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
