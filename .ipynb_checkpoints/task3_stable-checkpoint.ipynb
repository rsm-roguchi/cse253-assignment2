{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec59beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.models.pretrained import get_pretrained_model\n",
    "\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f01e14b-38ce-4419-8942-2f54f816faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/stable-audio-tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a08fe1-b112-4d0f-b922-94d7506a9db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2428f9e4-3b28-4c5c-b62c-b3050c1adfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Encoding and saving pre-encoded latent segments...\n",
      "‚úÖ Done. Saved 80 pre-encoded .npy segments to: /workspace/data3_preencoded_overlap\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Make sure model has a pretransform encoder\n",
    "assert hasattr(model, \"pretransform\") and model.pretransform is not None, \"Your model must have a .pretransform encoder\"\n",
    "model.pretransform.to(\"cuda\").eval()\n",
    "\n",
    "# Config\n",
    "AUDIO_DIR = Path(\"/workspace/data3\")\n",
    "OUTPUT_DIR = Path(\"/workspace/data3_preencoded_overlap\")\n",
    "SAMPLE_RATE = 44100\n",
    "SEGMENT_DURATION = 1.49\n",
    "SEGMENT_SAMPLES = int(SAMPLE_RATE * SEGMENT_DURATION)\n",
    "OVERLAP_RATIO = 0.5\n",
    "STEP_SIZE = int(SEGMENT_SAMPLES * (1 - OVERLAP_RATIO))\n",
    "MAX_SEGMENTS_PER_FILE = 20\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîÅ Encoding and saving pre-encoded latent segments...\")\n",
    "\n",
    "total_saved = 0\n",
    "\n",
    "for file in AUDIO_DIR.glob(\"*.wav\"):\n",
    "    try:\n",
    "        audio, sr = torchaudio.load(str(file))\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "            audio = resampler(audio)\n",
    "\n",
    "        # üîß Force stereo if mono\n",
    "        if audio.shape[0] == 1:\n",
    "            audio = torch.cat([audio, audio], dim=0)\n",
    "\n",
    "        start = 0\n",
    "        seg_idx = 0\n",
    "        while start + SEGMENT_SAMPLES <= audio.shape[1] and seg_idx < MAX_SEGMENTS_PER_FILE:\n",
    "            segment = audio[:, start:start + SEGMENT_SAMPLES].unsqueeze(0).to(\"cuda\")  # [1, 2, T]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latent = model.pretransform.encode(segment)  # [1, D, T']\n",
    "                latent = latent.squeeze(0).cpu().numpy()    # [D, T']\n",
    "\n",
    "            out_path = OUTPUT_DIR / f\"{file.stem}_ov{seg_idx}.npy\"\n",
    "            np.save(out_path, latent)\n",
    "\n",
    "            start += STEP_SIZE\n",
    "            seg_idx += 1\n",
    "            total_saved += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file.name}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Done. Saved {total_saved} pre-encoded .npy segments to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449e9d38-8c2c-4949-ba91-11c85d320170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import create_dataloader_from_config\n",
    "import json\n",
    "\n",
    "with open(\"/workspace/dataset_config.json\") as f:\n",
    "    dataset_config = json.load(f)\n",
    "\n",
    "train_loader = create_dataloader_from_config(\n",
    "    dataset_config,\n",
    "    batch_size=1,\n",
    "    sample_size=65536,\n",
    "    sample_rate=44100,\n",
    "    audio_channels=1,\n",
    "    num_workers=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de04a422-8da4-435f-aa49-dee9fa6ddf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/amp.py:54: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                             | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | diffusion     | ConditionedDiffusionModelWrapper | 497 M \n",
      "1 | diffusion_ema | EMA                              | 340 M \n",
      "2 | losses        | MultiLoss                        | 0     \n",
      "-------------------------------------------------------------------\n",
      "341 M     Trainable params\n",
      "496 M     Non-trainable params\n",
      "838 M     Total params\n",
      "3,352.279 Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (80) is smaller than the logging interval Trainer(log_every_n_steps=600). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1f952e2dde407da98a0c36b70cb36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/stable-audio-tools/stable_audio_tools/models/conditioners.py:362: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16) and torch.set_grad_enabled(self.enable_grad):\n",
      "`Trainer.fit` stopped: `max_steps=1200` reached.\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.training.diffusion import DiffusionCondTrainingWrapper\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "    model=model,\n",
    "    lr=1e-4,\n",
    "    pre_encoded=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=1200,                     # <-- total steps to train\n",
    "    accumulate_grad_batches=4,          # <-- simulate larger batch\n",
    "    precision=16,                        # <-- mixed precision (faster)\n",
    "    log_every_n_steps=600,               # <-- print loss every 10 steps\n",
    "    enable_progress_bar=True,           \n",
    "    enable_checkpointing=False,\n",
    "    val_check_interval=None,\n",
    "    strategy='auto',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(training_wrapper, train_dataloaders=train_loader)\n",
    "training_wrapper.export_model(\"/workspace/stable-audio-tools/saved/final_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fa03ed-63cf-4f9f-a0c1-2a3590001f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33636b34-132f-4cd1-90b6-9dbcb43ef4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725299224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:08,  7.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "conditioning = [\n",
    "    {\n",
    "        \"prompt\": \"Atlanta-style trap beat with heavy bass and synth lead\",\n",
    "        \"seconds_total\": 20.0  # leave this as float\n",
    "    }\n",
    "]\n",
    "\n",
    "DURATION_SEC = 10\n",
    "SAMPLE_RATE = 44100\n",
    "SAMPLE_SIZE = DURATION_SEC * SAMPLE_RATE\n",
    "\n",
    "output = generate_diffusion_cond(\n",
    "    model=model.to(\"cuda\"),       # make 100% sure model is on GPU\n",
    "    steps=1000,\n",
    "    cfg_scale=2.0,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=SAMPLE_SIZE,       # 9 seconds\n",
    "    device=\"cuda\"                 # force everything onto GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e328b845-8617-47be-9291-7d975c8af7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from einops import rearrange\n",
    "\n",
    "# Rearrange: [B, C, T] -> [C, T] for saving\n",
    "waveform = rearrange(output, \"b c n -> c (b n)\")\n",
    "\n",
    "# Peak normalize and convert to 16-bit PCM\n",
    "waveform = waveform.to(torch.float32).div(torch.max(torch.abs(waveform))).mul(32767).clamp(-32768, 32767).to(torch.int16).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "278da9b7-c3d2-49f8-8a27-9fa49d5e5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to remix_output_prompt_1000_steps.wav\n"
     ]
    }
   ],
   "source": [
    "torchaudio.save(\"remix_output_prompt_1000_steps.wav\", waveform, sample_rate=44100)\n",
    "print(\"‚úÖ Saved to remix_output_prompt_1000_steps.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
