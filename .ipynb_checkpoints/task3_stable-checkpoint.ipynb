{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec59beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.models.pretrained import get_pretrained_model\n",
    "\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f01e14b-38ce-4419-8942-2f54f816faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/stable-audio-tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a08fe1-b112-4d0f-b922-94d7506a9db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2428f9e4-3b28-4c5c-b62c-b3050c1adfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Encoding and saving pre-encoded latent segments...\n",
      "‚úÖ Done. Saved 120 pre-encoded .npy segments to: /workspace/data3_preencoded_overlap\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Make sure model has a pretransform encoder\n",
    "assert hasattr(model, \"pretransform\") and model.pretransform is not None, \"Your model must have a .pretransform encoder\"\n",
    "model.pretransform.to(\"cuda\").eval()\n",
    "\n",
    "# Config\n",
    "AUDIO_DIR = Path(\"/workspace/data3\")\n",
    "OUTPUT_DIR = Path(\"/workspace/data3_preencoded_overlap\")\n",
    "SAMPLE_RATE = 44100                  # Keep as-is\n",
    "SEGMENT_DURATION = 4.0               # Increased from 1.49 to 4 seconds\n",
    "SEGMENT_SAMPLES = int(SAMPLE_RATE * SEGMENT_DURATION)\n",
    "\n",
    "OVERLAP_RATIO = 0.75                 # Increased overlap from 50% to 75%\n",
    "STEP_SIZE = int(SEGMENT_SAMPLES * (1 - OVERLAP_RATIO))\n",
    "\n",
    "# Remove hard limit on segments per file\n",
    "# MAX_SEGMENTS_PER_FILE = 20  <-- DELETE THIS\n",
    "\n",
    "MAX_SEGMENTS_PER_FILE = 30\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîÅ Encoding and saving pre-encoded latent segments...\")\n",
    "\n",
    "total_saved = 0\n",
    "\n",
    "for file in AUDIO_DIR.glob(\"*.wav\"):\n",
    "    try:\n",
    "        audio, sr = torchaudio.load(str(file))\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "            audio = resampler(audio)\n",
    "\n",
    "        # üîß Force stereo if mono\n",
    "        if audio.shape[0] == 1:\n",
    "            audio = torch.cat([audio, audio], dim=0)\n",
    "\n",
    "        start = 0\n",
    "        seg_idx = 0\n",
    "        while start + SEGMENT_SAMPLES <= audio.shape[1] and seg_idx < MAX_SEGMENTS_PER_FILE:\n",
    "            segment = audio[:, start:start + SEGMENT_SAMPLES].unsqueeze(0).to(\"cuda\")  # [1, 2, T]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latent = model.pretransform.encode(segment)  # [1, D, T']\n",
    "                latent = latent.squeeze(0).cpu().numpy()    # [D, T']\n",
    "\n",
    "            out_path = OUTPUT_DIR / f\"{file.stem}_ov{seg_idx}.npy\"\n",
    "            np.save(out_path, latent)\n",
    "\n",
    "            start += STEP_SIZE\n",
    "            seg_idx += 1\n",
    "            total_saved += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file.name}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Done. Saved {total_saved} pre-encoded .npy segments to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449e9d38-8c2c-4949-ba91-11c85d320170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import create_dataloader_from_config\n",
    "import json\n",
    "\n",
    "with open(\"/workspace/dataset_config.json\") as f:\n",
    "    dataset_config = json.load(f)\n",
    "\n",
    "train_loader = create_dataloader_from_config(\n",
    "    dataset_config,\n",
    "    batch_size=1,\n",
    "    sample_size=176400,\n",
    "    sample_rate=44100,\n",
    "    audio_channels=1,\n",
    "    num_workers=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fa03ed-63cf-4f9f-a0c1-2a3590001f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de04a422-8da4-435f-aa49-dee9fa6ddf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/amp.py:54: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                             | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | diffusion     | ConditionedDiffusionModelWrapper | 497 M \n",
      "1 | diffusion_ema | EMA                              | 340 M \n",
      "2 | losses        | MultiLoss                        | 0     \n",
      "-------------------------------------------------------------------\n",
      "341 M     Trainable params\n",
      "496 M     Non-trainable params\n",
      "838 M     Total params\n",
      "3,352.279 Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (120) is smaller than the logging interval Trainer(log_every_n_steps=600). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58a94c3485a4e52b50cefba2e92d911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/stable-audio-tools/stable_audio_tools/models/conditioners.py:362: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16) and torch.set_grad_enabled(self.enable_grad):\n",
      "`Trainer.fit` stopped: `max_steps=1200` reached.\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.training.diffusion import DiffusionCondTrainingWrapper\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "    model=model,\n",
    "    lr=1e-4,\n",
    "    pre_encoded=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=1200,                     # <-- total steps to train\n",
    "    accumulate_grad_batches=4,          # <-- simulate larger batch\n",
    "    precision=16,                        # <-- mixed precision (faster)\n",
    "    log_every_n_steps=600,               # <-- print loss every 10 steps\n",
    "    enable_progress_bar=True,           \n",
    "    enable_checkpointing=False,\n",
    "    val_check_interval=None,\n",
    "    strategy='auto',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(training_wrapper, train_dataloaders=train_loader)\n",
    "training_wrapper.export_model(\"/workspace/stable-audio-tools/saved/final_model.pt\")\n",
    "\n",
    "with open(\"/workspace/stable-audio-tools/saved/final_model_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c8d53b-1bea-46d0-a080-e8add4159b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConditionedDiffusionModelWrapper(\n",
       "  (model): DiTWrapper(\n",
       "    (model): DiffusionTransformer(\n",
       "      (timestep_features): FourierFeatures()\n",
       "      (to_timestep_embed): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (to_cond_embed): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1024, bias=False)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (to_global_embed): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1024, bias=False)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (transformer): ContinuousTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x TransformerBlock(\n",
       "            (pre_norm): LayerNorm()\n",
       "            (self_attn): Attention(\n",
       "              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (k_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "            (self_attn_scale): Identity()\n",
       "            (cross_attend_norm): LayerNorm()\n",
       "            (cross_attn): Attention(\n",
       "              (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (k_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "            (cross_attn_scale): Identity()\n",
       "            (ff_norm): LayerNorm()\n",
       "            (ff): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): GLU(\n",
       "                  (act): SiLU()\n",
       "                  (proj): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (3): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ff_scale): Identity()\n",
       "          )\n",
       "        )\n",
       "        (project_in): Linear(in_features=64, out_features=1024, bias=False)\n",
       "        (project_out): Linear(in_features=1024, out_features=64, bias=False)\n",
       "        (rotary_pos_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (preprocess_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (postprocess_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (conditioner): MultiConditioner(\n",
       "    (conditioners): ModuleDict(\n",
       "      (prompt): T5Conditioner(\n",
       "        (proj_out): Identity()\n",
       "      )\n",
       "      (seconds_total): NumberConditioner(\n",
       "        (proj_out): Identity()\n",
       "        (embedder): NumberEmbedder(\n",
       "          (embedding): Sequential(\n",
       "            (0): LearnedPositionalEmbedding()\n",
       "            (1): Linear(in_features=257, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pretransform): AutoencoderPretransform(\n",
       "    (model): AudioAutoencoder(\n",
       "      (bottleneck): VAEBottleneck()\n",
       "      (encoder): OobleckEncoder(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv1d(2, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (3): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(256, 512, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (4): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "            )\n",
       "          )\n",
       "          (5): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(1024, 2048, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "            )\n",
       "          )\n",
       "          (6): SnakeBeta()\n",
       "          (7): Conv1d(2048, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (decoder): OobleckDecoder(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv1d(64, 2048, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(2048, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(1024, 512, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(512, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): SnakeBeta()\n",
       "          (7): Conv1d(128, 2, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "          (8): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from stable_audio_tools.models.diffusion import create_diffusion_cond_from_config\n",
    "\n",
    "# Load saved config\n",
    "with open(\"/workspace/stable-audio-tools/saved/final_model_config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Rebuild model from config\n",
    "model = create_diffusion_cond_from_config(config)\n",
    "ckpt = torch.load(\"/workspace/stable-audio-tools/saved/final_model.pt\", map_location=\"cuda\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.to(\"cuda\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33636b34-132f-4cd1-90b6-9dbcb43ef4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146676321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/stable-audio-tools/stable_audio_tools/models/conditioners.py:362: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16) and torch.set_grad_enabled(self.enable_grad):\n",
      "5000it [1:42:11,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "conditioning = [\n",
    "    {\n",
    "        \"prompt\": \"High-energy rap remix with gliding 808s, syncopated piano stabs, and crisp trap percussion\",\n",
    "        \"seconds_total\": 90.0  # leave this as float\n",
    "    }\n",
    "]\n",
    "\n",
    "DURATION_SEC = 90\n",
    "SAMPLE_RATE = 44100\n",
    "SAMPLE_SIZE = DURATION_SEC * SAMPLE_RATE\n",
    "\n",
    "\n",
    "\n",
    "output = generate_diffusion_cond(\n",
    "    model=model.to(\"cuda\"),       # make 100% sure model is on GPU\n",
    "    steps=5000,\n",
    "    cfg_scale=5.0,\n",
    "    conditioning=conditioning,\n",
    "    sample_size=SAMPLE_SIZE,       # 9 seconds\n",
    "    device=\"cuda\"                 # force everything onto GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e328b845-8617-47be-9291-7d975c8af7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from einops import rearrange\n",
    "\n",
    "# Rearrange: [B, C, T] -> [C, T] for saving\n",
    "waveform = rearrange(output, \"b c n -> c (b n)\")\n",
    "\n",
    "# Peak normalize and convert to 16-bit PCM\n",
    "waveform = waveform.to(torch.float32).div(torch.max(torch.abs(waveform))).mul(32767).clamp(-32768, 32767).to(torch.int16).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278da9b7-c3d2-49f8-8a27-9fa49d5e5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to remix_output_prompt_5000_steps.wav\n"
     ]
    }
   ],
   "source": [
    "torchaudio.save(\"remix_output_prompt_5000_steps.wav\", waveform, sample_rate=44100)\n",
    "print(\"‚úÖ Saved to remix_output_prompt_5000_steps.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
