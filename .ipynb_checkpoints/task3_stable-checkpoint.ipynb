{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec59beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.models.pretrained import get_pretrained_model\n",
    "\n",
    "model, model_config = get_pretrained_model(\"stabilityai/stable-audio-open-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f01e14b-38ce-4419-8942-2f54f816faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/stable-audio-tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a08fe1-b112-4d0f-b922-94d7506a9db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2428f9e4-3b28-4c5c-b62c-b3050c1adfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Encoding and saving pre-encoded latent segments...\n",
      "‚úÖ Done. Saved 120 pre-encoded .npy segments to: /workspace/data3_preencoded_overlap\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Make sure model has a pretransform encoder\n",
    "assert hasattr(model, \"pretransform\") and model.pretransform is not None, \"Your model must have a .pretransform encoder\"\n",
    "model.pretransform.to(\"cuda\").eval()\n",
    "\n",
    "# Config\n",
    "AUDIO_DIR = Path(\"/workspace/data3\")\n",
    "OUTPUT_DIR = Path(\"/workspace/data3_preencoded_overlap\")\n",
    "SAMPLE_RATE = 44100                  # Keep as-is\n",
    "SEGMENT_DURATION = 4.0               # Increased from 1.49 to 4 seconds\n",
    "SEGMENT_SAMPLES = int(SAMPLE_RATE * SEGMENT_DURATION)\n",
    "\n",
    "OVERLAP_RATIO = 0.75                 # Increased overlap from 50% to 75%\n",
    "STEP_SIZE = int(SEGMENT_SAMPLES * (1 - OVERLAP_RATIO))\n",
    "\n",
    "# Remove hard limit on segments per file\n",
    "# MAX_SEGMENTS_PER_FILE = 20  <-- DELETE THIS\n",
    "\n",
    "MAX_SEGMENTS_PER_FILE = 30\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üîÅ Encoding and saving pre-encoded latent segments...\")\n",
    "\n",
    "total_saved = 0\n",
    "\n",
    "for file in AUDIO_DIR.glob(\"*.wav\"):\n",
    "    try:\n",
    "        audio, sr = torchaudio.load(str(file))\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "            audio = resampler(audio)\n",
    "\n",
    "        # üîß Force stereo if mono\n",
    "        if audio.shape[0] == 1:\n",
    "            audio = torch.cat([audio, audio], dim=0)\n",
    "\n",
    "        start = 0\n",
    "        seg_idx = 0\n",
    "        while start + SEGMENT_SAMPLES <= audio.shape[1] and seg_idx < MAX_SEGMENTS_PER_FILE:\n",
    "            segment = audio[:, start:start + SEGMENT_SAMPLES].unsqueeze(0).to(\"cuda\")  # [1, 2, T]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latent = model.pretransform.encode(segment)  # [1, D, T']\n",
    "                latent = latent.squeeze(0).cpu().numpy()    # [D, T']\n",
    "\n",
    "            out_path = OUTPUT_DIR / f\"{file.stem}_ov{seg_idx}.npy\"\n",
    "            np.save(out_path, latent)\n",
    "\n",
    "            start += STEP_SIZE\n",
    "            seg_idx += 1\n",
    "            total_saved += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file.name}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Done. Saved {total_saved} pre-encoded .npy segments to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449e9d38-8c2c-4949-ba91-11c85d320170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 files\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.data.dataset import create_dataloader_from_config\n",
    "import json\n",
    "\n",
    "with open(\"/workspace/dataset_config.json\") as f:\n",
    "    dataset_config = json.load(f)\n",
    "\n",
    "train_loader = create_dataloader_from_config(\n",
    "    dataset_config,\n",
    "    batch_size=1,\n",
    "    sample_size=176400,\n",
    "    sample_rate=44100,\n",
    "    audio_channels=1,\n",
    "    num_workers=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fa03ed-63cf-4f9f-a0c1-2a3590001f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de04a422-8da4-435f-aa49-dee9fa6ddf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/amp.py:54: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                             | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | diffusion     | ConditionedDiffusionModelWrapper | 497 M \n",
      "1 | diffusion_ema | EMA                              | 340 M \n",
      "2 | losses        | MultiLoss                        | 0     \n",
      "-------------------------------------------------------------------\n",
      "341 M     Trainable params\n",
      "496 M     Non-trainable params\n",
      "838 M     Total params\n",
      "3,352.279 Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (120) is smaller than the logging interval Trainer(log_every_n_steps=600). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58a94c3485a4e52b50cefba2e92d911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/stable-audio-tools/stable_audio_tools/models/conditioners.py:362: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16) and torch.set_grad_enabled(self.enable_grad):\n",
      "`Trainer.fit` stopped: `max_steps=1200` reached.\n"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.training.diffusion import DiffusionCondTrainingWrapper\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "training_wrapper = DiffusionCondTrainingWrapper(\n",
    "    model=model,\n",
    "    lr=1e-4,\n",
    "    pre_encoded=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=1200,                     # <-- total steps to train\n",
    "    accumulate_grad_batches=4,          # <-- simulate larger batch\n",
    "    precision=16,                        # <-- mixed precision (faster)\n",
    "    log_every_n_steps=600,               # <-- print loss every 10 steps\n",
    "    enable_progress_bar=True,           \n",
    "    enable_checkpointing=False,\n",
    "    val_check_interval=None,\n",
    "    strategy='auto',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(training_wrapper, train_dataloaders=train_loader)\n",
    "training_wrapper.export_model(\"/workspace/stable-audio-tools/saved/final_model.pt\")\n",
    "\n",
    "with open(\"/workspace/stable-audio-tools/saved/final_model_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c8d53b-1bea-46d0-a080-e8add4159b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConditionedDiffusionModelWrapper(\n",
       "  (model): DiTWrapper(\n",
       "    (model): DiffusionTransformer(\n",
       "      (timestep_features): FourierFeatures()\n",
       "      (to_timestep_embed): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (to_cond_embed): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1024, bias=False)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (to_global_embed): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1024, bias=False)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (transformer): ContinuousTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x TransformerBlock(\n",
       "            (pre_norm): LayerNorm()\n",
       "            (self_attn): Attention(\n",
       "              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (k_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "            (self_attn_scale): Identity()\n",
       "            (cross_attend_norm): LayerNorm()\n",
       "            (cross_attn): Attention(\n",
       "              (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (q_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (k_norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "            (cross_attn_scale): Identity()\n",
       "            (ff_norm): LayerNorm()\n",
       "            (ff): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): GLU(\n",
       "                  (act): SiLU()\n",
       "                  (proj): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (3): Identity()\n",
       "              )\n",
       "            )\n",
       "            (ff_scale): Identity()\n",
       "          )\n",
       "        )\n",
       "        (project_in): Linear(in_features=64, out_features=1024, bias=False)\n",
       "        (project_out): Linear(in_features=1024, out_features=64, bias=False)\n",
       "        (rotary_pos_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (preprocess_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (postprocess_conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (conditioner): MultiConditioner(\n",
       "    (conditioners): ModuleDict(\n",
       "      (prompt): T5Conditioner(\n",
       "        (proj_out): Identity()\n",
       "      )\n",
       "      (seconds_total): NumberConditioner(\n",
       "        (proj_out): Identity()\n",
       "        (embedder): NumberEmbedder(\n",
       "          (embedding): Sequential(\n",
       "            (0): LearnedPositionalEmbedding()\n",
       "            (1): Linear(in_features=257, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pretransform): AutoencoderPretransform(\n",
       "    (model): AudioAutoencoder(\n",
       "      (bottleneck): VAEBottleneck()\n",
       "      (encoder): OobleckEncoder(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv1d(2, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (3): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(256, 512, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "            )\n",
       "          )\n",
       "          (4): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "            )\n",
       "          )\n",
       "          (5): EncoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (1): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): SnakeBeta()\n",
       "              (4): Conv1d(1024, 2048, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "            )\n",
       "          )\n",
       "          (6): SnakeBeta()\n",
       "          (7): Conv1d(2048, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (decoder): OobleckDecoder(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv1d(64, 2048, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(2048, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(1024, 1024, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(1024, 512, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(512, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): DecoderBlock(\n",
       "            (layers): Sequential(\n",
       "              (0): SnakeBeta()\n",
       "              (1): ConvTranspose1d(128, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              (2): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (3): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (4): ResidualUnit(\n",
       "                (layers): Sequential(\n",
       "                  (0): SnakeBeta()\n",
       "                  (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "                  (2): SnakeBeta()\n",
       "                  (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): SnakeBeta()\n",
       "          (7): Conv1d(128, 2, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "          (8): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from stable_audio_tools.models.diffusion import create_diffusion_cond_from_config\n",
    "\n",
    "# Load saved config\n",
    "with open(\"/workspace/stable-audio-tools/saved/final_model_config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Rebuild model from config\n",
    "model = create_diffusion_cond_from_config(config)\n",
    "ckpt = torch.load(\"/workspace/stable-audio-tools/saved/final_model.pt\", map_location=\"cuda\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.to(\"cuda\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33636b34-132f-4cd1-90b6-9dbcb43ef4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3757179291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (14) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m\n\u001b[1;32m     67\u001b[0m DURATION_SEC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds_total\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m conditioning)  \u001b[38;5;66;03m# = 90\u001b[39;00m\n\u001b[1;32m     68\u001b[0m SAMPLE_SIZE \u001b[38;5;241m=\u001b[39m SAMPLE_RATE \u001b[38;5;241m*\u001b[39m DURATION_SEC  \u001b[38;5;66;03m# = 3,969,000\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_diffusion_cond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# make 100% sure model is on GPU\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1536\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditioning_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditioning_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_conditioning_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_conditioning_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 9 seconds\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# force everything onto GPU\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/stable-audio-tools/stable_audio_tools/inference/generation.py:204\u001b[0m, in \u001b[0;36mgenerate_diffusion_cond\u001b[0;34m(model, steps, cfg_scale, conditioning, conditioning_tensors, negative_conditioning, negative_conditioning_tensors, batch_size, sample_size, sample_rate, seed, device, init_audio, init_noise_level, return_latents, **sampler_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sampler_kwargs:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m sampler_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 204\u001b[0m     sampled \u001b[38;5;241m=\u001b[39m \u001b[43msample_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconditioning_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnegative_conditioning_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_shift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# v-diffusion: \u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m#sampled = sample(model.model, noise, steps, 0, **conditioning_tensors, embedding_scale=cfg_scale)\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m noise\n",
      "File \u001b[0;32m/workspace/stable-audio-tools/stable_audio_tools/inference/sampling.py:451\u001b[0m, in \u001b[0;36msample_rf\u001b[0;34m(model_fn, noise, init_data, steps, sampler_type, sigma_max, device, callback, cond_fn, **extra_args)\u001b[0m\n\u001b[1;32m    448\u001b[0m t[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuler\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msample_discrete_euler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrk4\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_rk4(model_fn, x, steps, sigma_max, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/stable-audio-tools/stable_audio_tools/inference/sampling.py:126\u001b[0m, in \u001b[0;36msample_discrete_euler\u001b[0;34m(model, x, steps, sigma_max, sigmas, callback, dist_shift, disable_tqdm, **extra_args)\u001b[0m\n\u001b[1;32m    120\u001b[0m t_curr_tensor \u001b[38;5;241m=\u001b[39m t_curr \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[1;32m    121\u001b[0m     (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m dt \u001b[38;5;241m=\u001b[39m t_prev \u001b[38;5;241m-\u001b[39m t_curr  \u001b[38;5;66;03m# we solve backwards in our formulation\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_curr_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m v\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/stable-audio-tools/stable_audio_tools/models/diffusion.py:543\u001b[0m, in \u001b[0;36mDiTWrapper.forward\u001b[0;34m(self, x, t, cross_attn_cond, cross_attn_mask, negative_cross_attn_cond, negative_cross_attn_mask, input_concat_cond, negative_input_concat_cond, global_cond, negative_global_cond, prepend_cond, prepend_cond_mask, cfg_scale, cfg_dropout_prob, batch_cfg, rescale_cfg, scale_phi, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch_cfg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_cfg must be True for DiTWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m#assert negative_input_concat_cond is None, \"negative_input_concat_cond is not supported for DiTWrapper\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_cond_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_cross_attn_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_cross_attn_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_cross_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_cross_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_concat_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_concat_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_cond_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_cond_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_dropout_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_dropout_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_phi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_phi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/stable-audio-tools/stable_audio_tools/models/dit.py:356\u001b[0m, in \u001b[0;36mDiffusionTransformer.forward\u001b[0;34m(self, x, t, cross_attn_cond, cross_attn_cond_mask, negative_cross_attn_cond, negative_cross_attn_mask, input_concat_cond, global_embed, negative_global_embed, prepend_cond, prepend_cond_mask, cfg_scale, cfg_dropout_prob, cfg_interval, causal, scale_phi, mask, return_info, exit_layer_ix, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m negative_cross_attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         negative_cross_attn_mask \u001b[38;5;241m=\u001b[39m negative_cross_attn_mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 356\u001b[0m         negative_cross_attn_cond \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative_cross_attn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_cross_attn_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnull_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     batch_cond \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cross_attn_cond, negative_cross_attn_cond], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (14) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from stable_audio_tools.inference.generation import generate_diffusion_cond\n",
    "\n",
    "conditioning = [\n",
    "    {\n",
    "        \"prompt\": \"Intro: moody ambient piano with vinyl crackle and subtle noise\",\n",
    "        \"seconds_total\": 8\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Build-up: rising tension with reversed piano and filtered trap drums\",\n",
    "        \"seconds_total\": 8\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"First drop: heavy gliding 808s, crisp hi-hats, and chopped minor piano\",\n",
    "        \"seconds_total\": 18\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Bridge: minimal drums, echo FX, and ambient chord washes\",\n",
    "        \"seconds_total\": 10\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Second drop: aggressive 808 layers, syncopated hi-hats, reverb piano stabs\",\n",
    "        \"seconds_total\": 20\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Breakdown: filtered piano with distorted risers and low-end rumble\",\n",
    "        \"seconds_total\": 12\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Outro: ambient reverb decay, fading piano, and delay FX\",\n",
    "        \"seconds_total\": 14\n",
    "    }\n",
    "]\n",
    "\n",
    "negative_conditioning = [\n",
    "    {\n",
    "        \"prompt\": \"no repetitive loops, avoid ambient drone, do not fade out early, no silence\",\n",
    "        \"seconds_total\": 90.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the actual conditioning tensors\n",
    "conditioning_tensors = model.conditioner(conditioning, device=\"cuda\")\n",
    "negative_conditioning_tensors = model.conditioner(negative_conditioning, device=\"cuda\")\n",
    "\n",
    "# If using classifier-free guidance, double all tensors\n",
    "def double_conditioning_tensors(tensor_dict):\n",
    "    doubled = {}\n",
    "    for k, v in tensor_dict.items():\n",
    "        if isinstance(v, (list, tuple)) and isinstance(v[0], torch.Tensor):\n",
    "            v0, v1 = v\n",
    "            # Make sure both get repeated correctly\n",
    "            new_v0 = v0.repeat(2, *[1] * (v0.ndim - 1))\n",
    "            new_v1 = v1.repeat(2, *[1] * (v1.ndim - 1))\n",
    "            doubled[k] = [new_v0, new_v1]\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            doubled[k] = v.repeat(2, *[1] * (v.ndim - 1))\n",
    "        else:\n",
    "            doubled[k] = v\n",
    "    return doubled\n",
    "\n",
    "\n",
    "conditioning_tensors = double_conditioning_tensors(model.conditioner(conditioning, device=\"cuda\"))\n",
    "negative_conditioning_tensors = double_conditioning_tensors(model.conditioner(negative_conditioning, device=\"cuda\"))\n",
    "\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "DURATION_SEC = sum(p[\"seconds_total\"] for p in conditioning)  # = 90\n",
    "SAMPLE_SIZE = SAMPLE_RATE * DURATION_SEC  # = 3,969,000\n",
    "\n",
    "\n",
    "output = generate_diffusion_cond(\n",
    "    model=model.to(\"cuda\"),       # make 100% sure model is on GPU\n",
    "    steps=1536,\n",
    "    cfg_scale=6.5,\n",
    "    conditioning_tensors=conditioning_tensors,\n",
    "    negative_conditioning_tensors=negative_conditioning_tensors,\n",
    "    sample_size=SAMPLE_SIZE,       # 9 seconds\n",
    "    device=\"cuda\"                 # force everything onto GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e328b845-8617-47be-9291-7d975c8af7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from einops import rearrange\n",
    "\n",
    "# Rearrange: [B, C, T] -> [C, T] for saving\n",
    "waveform = rearrange(output, \"b c n -> c (b n)\")\n",
    "\n",
    "# Peak normalize and convert to 16-bit PCM\n",
    "waveform = waveform.to(torch.float32).div(torch.max(torch.abs(waveform))).mul(32767).clamp(-32768, 32767).to(torch.int16).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278da9b7-c3d2-49f8-8a27-9fa49d5e5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to remix_output_prompt_5000_steps.wav\n"
     ]
    }
   ],
   "source": [
    "torchaudio.save(\"remix_output_prompt_1536_steps.wav\", waveform, sample_rate=44100)\n",
    "print(\"‚úÖ Saved to remix_output_prompt_1536_steps.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
