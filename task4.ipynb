{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c715e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from multiprocessing import Manager\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import random\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.cuda.amp as amp\n",
    "from tqdm import tqdm, trange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cbd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNIPPET_SECONDS = 2\n",
    "SR = 22050\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 80\n",
    "BATCH_SIZE = 8\n",
    "LATENT_DIM = (1, N_MELS, 512)\n",
    "TRAIN_STEPS = 5000\n",
    "OPT_STEPS = 70\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4154ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "shared_snippet_log = manager.dict()  # key=(file_id, start), value=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560b46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FFT = 1024  # Make sure this matches your librosa calls\n",
    "\n",
    "class DynamicAudioMelodyDataset(Dataset):\n",
    "    def __init__(self, wav_paths, shared_log, snippet_duration=SNIPPET_SECONDS, sr=SR, epoch_samples=5000):\n",
    "        self.sr = sr\n",
    "        self.snippet_duration = max(snippet_duration, N_FFT / sr)\n",
    "        self.samples = max(int(self.sr * self.snippet_duration), N_FFT)\n",
    "        self.data = []\n",
    "        self.snippet_log = shared_log\n",
    "        self.epoch_samples = epoch_samples\n",
    "\n",
    "        for path in tqdm(wav_paths, desc=\"Preloading audio files\"):\n",
    "            y, _ = librosa.load(path, sr=sr)\n",
    "            if len(y) < self.samples:\n",
    "                y = np.pad(y, (0, self.samples - len(y)))\n",
    "            self.data.append(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of samples per epoch\n",
    "        return self.epoch_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = random.choice(self.data)\n",
    "        file_id = id(y)\n",
    "\n",
    "        # Always get exactly self.samples\n",
    "        start = random.randint(0, len(y) - self.samples)\n",
    "        snippet = y[start:start + self.samples]\n",
    "\n",
    "        # Defensive: should not be needed, but be paranoid\n",
    "        if len(snippet) < N_FFT:\n",
    "            snippet = np.pad(snippet, (0, N_FFT - len(snippet)))\n",
    "        elif len(snippet) > self.samples:\n",
    "            snippet = snippet[:self.samples]\n",
    "\n",
    "        self.snippet_log[(file_id, start)] = True\n",
    "\n",
    "        # Feature extraction (use float32 for PyTorch)\n",
    "        mel = librosa.feature.melspectrogram(y=snippet, sr=self.sr, n_mels=N_MELS)\n",
    "        mel_db = librosa.power_to_db(mel).astype(np.float32)\n",
    "        chroma = librosa.feature.chroma_cqt(y=snippet, sr=self.sr, hop_length=HOP_LENGTH)\n",
    "        chroma = librosa.decompose.nn_filter(chroma, aggregate=np.median)\n",
    "        melody = np.argmax(chroma, axis=0).astype(np.int64)\n",
    "\n",
    "        # Truncate to multiple of 16 for time dim\n",
    "        T = mel_db.shape[1]\n",
    "        T_16 = (T // 16) * 16\n",
    "        mel_db = mel_db[:, :T_16]\n",
    "        mel_tensor = torch.tensor(mel_db, dtype=torch.float32).unsqueeze(0)\n",
    "        melody = melody[:T_16]\n",
    "        melody_tensor = torch.tensor(melody, dtype=torch.long)\n",
    "\n",
    "        return mel_tensor, melody_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b028b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(tensor, target_shape):\n",
    "    # tensor: [B, C, H, W]\n",
    "    _, _, h, w = tensor.shape\n",
    "    target_h, target_w = target_shape\n",
    "    start_h = (h - target_h) // 2\n",
    "    start_w = (w - target_w) // 2\n",
    "    return tensor[:, :, start_h:start_h + target_h, start_w:start_w + target_w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eba8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb  # [B, dim]\n",
    "    \n",
    "\n",
    "class StrongUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_channels=64, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(in_channels, base_channels)\n",
    "        self.enc2 = self.conv_block(base_channels, base_channels * 2)\n",
    "        self.enc3 = self.conv_block(base_channels * 2, base_channels * 4)\n",
    "        self.enc4 = self.conv_block(base_channels * 4, base_channels * 8)\n",
    "\n",
    "        self.middle = self.conv_block(base_channels * 8, base_channels * 16)\n",
    "\n",
    "        self.dec4 = self.up_block(base_channels * 16, base_channels * 8)\n",
    "        self.dec3 = self.up_block(base_channels * 8 * 2, base_channels * 4)\n",
    "        self.dec2 = self.up_block(base_channels * 4 * 2, base_channels * 2)\n",
    "        self.dec1 = self.up_block(base_channels * 2 * 2, base_channels)\n",
    "\n",
    "        self.out = nn.Conv2d(base_channels * 2, in_channels, kernel_size=1)\n",
    "\n",
    "        # --- Time embedding modules ---\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim * 4, base_channels * 16),\n",
    "        )\n",
    "\n",
    "        # Project t_emb to each decoder block's channel size\n",
    "        self.tproj_dec4 = nn.Linear(base_channels * 16, base_channels * 8)\n",
    "        self.tproj_dec3 = nn.Linear(base_channels * 16, base_channels * 4)\n",
    "        self.tproj_dec2 = nn.Linear(base_channels * 16, base_channels * 2)\n",
    "        self.tproj_dec1 = nn.Linear(base_channels * 16, base_channels)\n",
    "\n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def up_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # --- Embed timestep ---\n",
    "        t_emb = self.time_mlp(t.float())  # [B, base_channels*16]\n",
    "\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(F.max_pool2d(e1, 2))\n",
    "        e3 = self.enc3(F.max_pool2d(e2, 2))\n",
    "        e4 = self.enc4(F.max_pool2d(e3, 2))\n",
    "\n",
    "        m = self.middle(F.max_pool2d(e4, 2))  # [B, base_channels*16, ...]\n",
    "\n",
    "        # --- Add t embedding to the bottleneck and decoders ---\n",
    "        # m: [B, C, H, W]\n",
    "        m = m + t_emb[:, :, None, None]  # broadcast [B, C, 1, 1]\n",
    "\n",
    "        d4 = self.dec4(m)\n",
    "        d4 = d4 + self.tproj_dec4(t_emb)[:, :, None, None]  # add t embedding\n",
    "        e4_c = center_crop(e4, d4.shape[2:])\n",
    "        d4 = torch.cat([d4, e4_c], dim=1)\n",
    "\n",
    "        d3 = self.dec3(d4)\n",
    "        d3 = d3 + self.tproj_dec3(t_emb)[:, :, None, None]\n",
    "        e3_c = center_crop(e3, d3.shape[2:])\n",
    "        d3 = torch.cat([d3, e3_c], dim=1)\n",
    "\n",
    "        d2 = self.dec2(d3)\n",
    "        d2 = d2 + self.tproj_dec2(t_emb)[:, :, None, None]\n",
    "        e2_c = center_crop(e2, d2.shape[2:])\n",
    "        d2 = torch.cat([d2, e2_c], dim=1)\n",
    "\n",
    "        d1 = self.dec1(d2)\n",
    "        d1 = d1 + self.tproj_dec1(t_emb)[:, :, None, None]\n",
    "        e1_c = center_crop(e1, d1.shape[2:])\n",
    "        d1 = torch.cat([d1, e1_c], dim=1)\n",
    "\n",
    "        return self.out(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248ac952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical linear beta schedule for DDPM\n",
    "def make_beta_schedule(T, beta_start=1e-4, beta_end=0.02):\n",
    "    return torch.linspace(beta_start, beta_end, T)\n",
    "\n",
    "T = 1000  # number of diffusion steps\n",
    "betas = make_beta_schedule(T)  # [T]\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)  # [T]\n",
    "\n",
    "def train_ddpm(model, dataloader, shared_log, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1500, gamma=0.5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "\n",
    "    T = 1000\n",
    "    betas = make_beta_schedule(T).to(device)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=TRAIN_STEPS, desc=\"Training DDPM\")\n",
    "    for step, (mel, _) in pbar:\n",
    "        if step >= TRAIN_STEPS:\n",
    "            break\n",
    "\n",
    "        mel = mel.to(device)  # shape: [B, ...]\n",
    "        B = mel.size(0)\n",
    "\n",
    "        # Sample random timesteps for each item in batch\n",
    "        t = torch.randint(0, T, (B,), device=device).long()  # [B]\n",
    "\n",
    "        # Get corresponding alphas for each t\n",
    "        alphas_cumprod_t = alphas_cumprod[t].view(B, 1, 1, 1)  # Adjust dimensions as needed for mel shape\n",
    "\n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(mel)\n",
    "        x_t = torch.sqrt(alphas_cumprod_t) * mel + torch.sqrt(1 - alphas_cumprod_t) * noise\n",
    "\n",
    "        # Predict noise (most common DDPM target)\n",
    "        pred_noise = model(x_t, t)\n",
    "\n",
    "        loss = loss_fn(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"unique\": len(shared_log)\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88836765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading audio files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading audio files: 100%|██████████| 4/4 [00:01<00:00,  2.65it/s]\n",
      "Training DDPM:   0%|          | 0/5000 [00:00<?, ?it/s]/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "Training DDPM:   0%|          | 0/5000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m model = StrongUNet().cuda()  \u001b[38;5;66;03m# <-- use the *patched* time-aware version\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# --- Train ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_log\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_snippet_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- Save ---\u001b[39;00m\n\u001b[32m     22\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints2\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_ddpm\u001b[39m\u001b[34m(model, dataloader, shared_log, device)\u001b[39m\n\u001b[32m     37\u001b[39m x_t = torch.sqrt(alphas_cumprod_t) * mel + torch.sqrt(\u001b[32m1\u001b[39m - alphas_cumprod_t) * noise\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Predict noise (most common DDPM target)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m pred_noise = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m loss = loss_fn(pred_noise, noise)\n\u001b[32m     44\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mStrongUNet.forward\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     70\u001b[39m e3 = \u001b[38;5;28mself\u001b[39m.enc3(F.max_pool2d(e2, \u001b[32m2\u001b[39m))\n\u001b[32m     71\u001b[39m e4 = \u001b[38;5;28mself\u001b[39m.enc4(F.max_pool2d(e3, \u001b[32m2\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmiddle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43me4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, base_channels*16, ...]\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# --- Add t embedding to the bottleneck and decoders ---\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# m: [B, C, H, W]\u001b[39;00m\n\u001b[32m     77\u001b[39m m = m + t_emb[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]  \u001b[38;5;66;03m# broadcast [B, C, 1, 1]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- WAV loading ---\n",
    "wav_dir = \"data3\"\n",
    "wav_files = [os.path.join(wav_dir, f) for f in os.listdir(wav_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "# --- Shared log for snippet tracking ---\n",
    "manager = Manager()\n",
    "shared_snippet_log = manager.dict()\n",
    "\n",
    "# --- Dataset and DataLoader ---\n",
    "dataset = DynamicAudioMelodyDataset(wav_files, shared_log=shared_snippet_log)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "\n",
    "# --- Import your StrongUNet (with time conditioning) ---\n",
    "# from your_model_file import StrongUNet\n",
    "\n",
    "model = StrongUNet().cuda()  # <-- use the *patched* time-aware version\n",
    "\n",
    "# --- Train ---\n",
    "train_ddpm(model, dataloader, shared_log=shared_snippet_log, device=torch.device(\"cuda\"))\n",
    "\n",
    "# --- Save ---\n",
    "os.makedirs(\"checkpoints2\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"checkpoints2/trained_ddpm_melody.pt\")\n",
    "print(\"Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f29b57",
   "metadata": {},
   "source": [
    "# Vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c11d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiFiGANGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mel_conv = nn.Conv1d(80, 512, 7, padding=3)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose1d(512, 256, 16, stride=8, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(256, 128, 16, stride=8, padding=4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, 7, padding=3)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel):\n",
    "        x = self.mel_conv(mel)\n",
    "        return self.upsample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelAudioDataset(Dataset):\n",
    "    def __init__(self, wav_paths, segment_duration=2.0, sr=22050):\n",
    "        self.sr = sr\n",
    "        self.segment_samples = int(segment_duration * sr)\n",
    "        self.wav_paths = wav_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_paths) * 20\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = random.choice(self.wav_paths)\n",
    "        audio, _ = librosa.load(path, sr=self.sr)\n",
    "        if len(audio) < self.segment_samples:\n",
    "            audio = np.pad(audio, (0, self.segment_samples - len(audio)))\n",
    "        start = random.randint(0, len(audio) - self.segment_samples)\n",
    "        segment = audio[start:start + self.segment_samples]\n",
    "        mel = librosa.feature.melspectrogram(y=segment, sr=self.sr, n_fft=1024, hop_length=256, n_mels=80)\n",
    "        mel = librosa.power_to_db(mel).astype(np.float32)\n",
    "        mel_tensor = torch.tensor(mel)\n",
    "        audio_tensor = torch.tensor(segment).unsqueeze(0)  # [1, T]\n",
    "        return mel_tensor, audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hifigan(generator, dataloader, epochs=20, lr=2e-4, step_size=5, gamma=0.5):\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(gen_opt, step_size=step_size, gamma=gamma)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    generator.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, (mel, audio) in enumerate(pbar):\n",
    "            mel = mel.to(\"cuda\")           # [B, 80, T]\n",
    "            audio = audio.to(\"cuda\")       # [B, 1, samples]\n",
    "            gen_out = generator(mel)       # [B, 1, samples]\n",
    "\n",
    "            min_len = min(gen_out.shape[-1], audio.shape[-1])\n",
    "            loss = loss_fn(gen_out[..., :min_len], audio[..., :min_len])\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (step + 1)\n",
    "            pbar.set_postfix({\"avg_loss\": f\"{avg_loss:.4f}\", \"lr\": gen_opt.param_groups[0]['lr']})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9776c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_concat_mel(original_audio, remix_audio, sr=22050):\n",
    "    concat_audio = np.concatenate([original_audio, remix_audio])\n",
    "    S_concat = librosa.feature.melspectrogram(y=concat_audio, sr=sr, n_fft=2048, hop_length=512, n_mels=80)\n",
    "    S_db_concat = librosa.power_to_db(S_concat, ref=np.max)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    img = librosa.display.specshow(S_db_concat, sr=sr, hop_length=512, y_axis='mel', x_axis='time', cmap='magma')\n",
    "    plt.title(\"Concatenated (Original + Remix) Mel-Spectrogram\")\n",
    "    plt.colorbar(img, format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def griffin_lim_vocoder(mel_spec, sr=22050, n_iter=32):\n",
    "    mel = mel_spec.squeeze(0).detach().cpu().numpy()\n",
    "    db = librosa.db_to_power(mel)\n",
    "    audio = librosa.feature.inverse.mel_to_audio(db, sr=sr, n_fft=2048, hop_length=512, n_iter=n_iter)\n",
    "    return audio\n",
    "\n",
    "def extract_pitch_classes(audio, sr=22050, hop_length=512):\n",
    "    chroma = librosa.feature.chroma_cqt(y=audio, sr=sr, hop_length=hop_length)\n",
    "    chroma = librosa.decompose.nn_filter(chroma, aggregate=np.median)\n",
    "    labels = np.argmax(chroma, axis=0)\n",
    "    return torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def ancestral_sample(model, x_T, steps=50):\n",
    "    x = x_T.clone()\n",
    "    if x.dim() == 3:\n",
    "        x = x.unsqueeze(0)  # Ensure [1, 80, T]\n",
    "    for _ in trange(steps, desc=\"Sampling\"):\n",
    "        noise = model(x)\n",
    "        x = x - 0.1 * noise\n",
    "    return x\n",
    "\n",
    "\n",
    "def ditto_optimize_latent(model, vocoder, y_target, target_len_samples, sr=22050, steps=70, latent_shape=(1, 80, 512)):\n",
    "    x_T = torch.randn(latent_shape, device='cuda', requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([x_T], lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.7)  # decay every 30 steps\n",
    "\n",
    "    y_target = y_target.to(x_T.device)\n",
    "    project = torch.nn.Linear(latent_shape[1], 12).to(x_T.device)  # Only ONCE before the loop\n",
    "\n",
    "    for step in range(steps):\n",
    "        x_0 = ancestral_sample(model, x_T, steps=10)  # [1, 80, T] or [1, 1, 80, T]\n",
    "        chroma_like = torch.nn.functional.normalize(x_0, dim=1)\n",
    "        if chroma_like.dim() == 4:\n",
    "            logits = chroma_like.squeeze(0).squeeze(0)   # [80, T]\n",
    "        elif chroma_like.dim() == 3:\n",
    "            logits = chroma_like.squeeze(0)              # [80, T]\n",
    "        else:\n",
    "            logits = chroma_like\n",
    "        logits = logits.permute(1, 0)  # [T, 80], always safe\n",
    "        y_hat = project(logits)  # [T, 12]\n",
    "\n",
    "        min_len = min(len(y_target), y_hat.shape[0])\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat[:min_len], y_target[:min_len])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        torch.cuda.empty_cache()  # optional, but helps avoid CUDA memory issues\n",
    "\n",
    "    x_final = ancestral_sample(model, x_T.detach(), steps=30)  # [1, 80, T]\n",
    "    # x_final: [1, 80, T] or [1, 1, 80, T]\n",
    "    if x_final.dim() == 4:\n",
    "        mel_final = x_final.squeeze(0).squeeze(0)  # [80, T]\n",
    "    elif x_final.dim() == 3:\n",
    "        mel_final = x_final.squeeze(0)             # [80, T]\n",
    "    else:\n",
    "        mel_final = x_final\n",
    "\n",
    "    mel_final = mel_final.unsqueeze(0)  # [1, 80, T]\n",
    "    audio = vocoder(mel_final).squeeze().detach().cpu().numpy()  # [T]\n",
    "\n",
    "\n",
    "    # 🔧 Adjust output length to match original\n",
    "    if len(audio) > target_len_samples:\n",
    "        audio = audio[:target_len_samples]\n",
    "    elif len(audio) < target_len_samples:\n",
    "        audio = np.pad(audio, (0, target_len_samples - len(audio)))\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_snippet(wav_path, duration_sec=5, sr=22050):\n",
    "    y, _ = librosa.load(wav_path, sr=sr)\n",
    "    total_samples = len(y)\n",
    "    snippet_len = sr * duration_sec\n",
    "    if total_samples < snippet_len:\n",
    "        y = np.pad(y, (0, snippet_len - total_samples))\n",
    "    start = random.randint(0, max(1, total_samples - snippet_len))\n",
    "    return y[start:start + snippet_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [00:01<00:00,  5.82it/s, avg_loss=0.2127, lr=0.0002]\n",
      "Epoch 2: 100%|██████████| 10/10 [00:01<00:00,  6.03it/s, avg_loss=0.0910, lr=0.0002]\n",
      "Epoch 3: 100%|██████████| 10/10 [00:01<00:00,  5.94it/s, avg_loss=0.0875, lr=0.0002]\n",
      "Epoch 4: 100%|██████████| 10/10 [00:01<00:00,  5.82it/s, avg_loss=0.0885, lr=0.0002]\n",
      "Epoch 5: 100%|██████████| 10/10 [00:01<00:00,  6.16it/s, avg_loss=0.0824, lr=0.0002]\n",
      "Epoch 6: 100%|██████████| 10/10 [00:01<00:00,  5.88it/s, avg_loss=0.0833, lr=0.0001]\n",
      "Epoch 7: 100%|██████████| 10/10 [00:01<00:00,  5.75it/s, avg_loss=0.0823, lr=0.0001]\n",
      "Epoch 8: 100%|██████████| 10/10 [00:01<00:00,  5.56it/s, avg_loss=0.0795, lr=0.0001]\n",
      "Epoch 9: 100%|██████████| 10/10 [00:01<00:00,  6.03it/s, avg_loss=0.0813, lr=0.0001]\n",
      "Epoch 10: 100%|██████████| 10/10 [00:01<00:00,  5.67it/s, avg_loss=0.0799, lr=0.0001]\n",
      "Epoch 11: 100%|██████████| 10/10 [00:01<00:00,  6.11it/s, avg_loss=0.0908, lr=5e-5]\n",
      "Epoch 12: 100%|██████████| 10/10 [00:01<00:00,  6.08it/s, avg_loss=0.0901, lr=5e-5]\n",
      "Epoch 13: 100%|██████████| 10/10 [00:01<00:00,  5.45it/s, avg_loss=0.0883, lr=5e-5]\n",
      "Epoch 14: 100%|██████████| 10/10 [00:01<00:00,  6.38it/s, avg_loss=0.0824, lr=5e-5]\n",
      "Epoch 15: 100%|██████████| 10/10 [00:01<00:00,  6.15it/s, avg_loss=0.0848, lr=5e-5]\n",
      "Epoch 16: 100%|██████████| 10/10 [00:01<00:00,  6.19it/s, avg_loss=0.0834, lr=2.5e-5]\n",
      "Epoch 17: 100%|██████████| 10/10 [00:01<00:00,  5.74it/s, avg_loss=0.0882, lr=2.5e-5]\n",
      "Epoch 18: 100%|██████████| 10/10 [00:01<00:00,  5.79it/s, avg_loss=0.0756, lr=2.5e-5]\n",
      "Epoch 19: 100%|██████████| 10/10 [00:01<00:00,  6.33it/s, avg_loss=0.0811, lr=2.5e-5]\n",
      "Epoch 20: 100%|██████████| 10/10 [00:01<00:00,  6.24it/s, avg_loss=0.0794, lr=2.5e-5]\n"
     ]
    }
   ],
   "source": [
    "wav_dir = \"data3\"\n",
    "wav_files = [os.path.join(wav_dir, f) for f in os.listdir(wav_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "dataset = MelAudioDataset(wav_files)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=16)\n",
    "\n",
    "generator = HiFiGANGenerator().cuda()\n",
    "trained_gen = train_hifigan(generator, dataloader, epochs=20)\n",
    "\n",
    "torch.save(trained_gen.state_dict(), \"checkpoints2/trained_hifigan.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae34d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roguchi/cse253-assignment2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 617.15it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 391.03it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 393.73it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 389.79it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 402.86it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 405.40it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 402.64it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 387.06it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 393.92it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 390.73it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 390.22it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 394.85it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 395.68it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 404.31it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 403.76it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 409.95it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 398.80it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 402.77it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 407.48it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 403.44it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 399.42it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 400.38it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 394.35it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 402.53it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 400.41it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 402.16it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 400.61it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 394.81it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 404.71it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 407.56it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 402.29it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 400.01it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 391.89it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 394.29it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 403.06it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 372.55it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 397.58it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 370.46it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 395.26it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 393.88it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 392.21it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 393.94it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 396.26it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 405.49it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 400.00it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 392.64it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 399.78it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 386.53it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 392.43it/s]\n",
      "Sampling: 100%|██████████| 10/10 [00:00<00:00, 404.71it/s]\n",
      "Sampling: 100%|██████████| 30/30 [00:00<00:00, 128.22it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.62 GiB of which 13.06 MiB is free. Process 4015399 has 52.00 MiB memory in use. Process 4015398 has 52.00 MiB memory in use. Process 4015428 has 52.00 MiB memory in use. Process 4015364 has 52.00 MiB memory in use. Process 4015464 has 52.00 MiB memory in use. Process 4015456 has 52.00 MiB memory in use. Process 4015383 has 52.00 MiB memory in use. Process 4015405 has 52.00 MiB memory in use. Process 4015403 has 52.00 MiB memory in use. Process 4015440 has 52.00 MiB memory in use. Process 4015373 has 52.00 MiB memory in use. Process 4015259 has 52.00 MiB memory in use. Process 4015475 has 52.00 MiB memory in use. Process 4015202 has 52.00 MiB memory in use. Process 4015471 has 52.00 MiB memory in use. Process 4015427 has 52.00 MiB memory in use. Process 4015378 has 52.00 MiB memory in use. Process 4015451 has 52.00 MiB memory in use. Process 4015460 has 52.00 MiB memory in use. Process 4015248 has 52.00 MiB memory in use. Process 4015457 has 52.00 MiB memory in use. Process 4015363 has 52.00 MiB memory in use. Process 4015397 has 52.00 MiB memory in use. Process 4015458 has 52.00 MiB memory in use. Process 4015472 has 52.00 MiB memory in use. Process 4015241 has 52.00 MiB memory in use. Process 4015237 has 52.00 MiB memory in use. Process 4015454 has 52.00 MiB memory in use. Process 4015366 has 52.00 MiB memory in use. Process 4015453 has 52.00 MiB memory in use. Process 4015465 has 52.00 MiB memory in use. Process 4015367 has 52.00 MiB memory in use. Process 4015244 has 52.00 MiB memory in use. Process 4015400 has 52.00 MiB memory in use. Process 4015239 has 52.00 MiB memory in use. Process 4015402 has 52.00 MiB memory in use. Process 4015371 has 52.00 MiB memory in use. Process 4015242 has 52.00 MiB memory in use. Process 4015201 has 52.00 MiB memory in use. Process 4015474 has 52.00 MiB memory in use. Process 4015265 has 52.00 MiB memory in use. Process 4015455 has 52.00 MiB memory in use. Process 4015382 has 52.00 MiB memory in use. Process 4015525 has 52.00 MiB memory in use. Process 4015384 has 52.00 MiB memory in use. Process 4015249 has 52.00 MiB memory in use. Process 4015240 has 52.00 MiB memory in use. Process 4015243 has 52.00 MiB memory in use. Process 4015432 has 52.00 MiB memory in use. Process 4015395 has 52.00 MiB memory in use. Process 4015329 has 52.00 MiB memory in use. Process 4015365 has 52.00 MiB memory in use. Process 4015433 has 52.00 MiB memory in use. Process 4015404 has 52.00 MiB memory in use. Process 4015206 has 52.00 MiB memory in use. Process 4015266 has 52.00 MiB memory in use. Process 4015396 has 52.00 MiB memory in use. Process 4015469 has 52.00 MiB memory in use. Process 4015459 has 52.00 MiB memory in use. Process 4015444 has 52.00 MiB memory in use. Process 4015256 has 52.00 MiB memory in use. Process 4015359 has 52.00 MiB memory in use. Process 4015213 has 52.00 MiB memory in use. Process 2307265 has 1.65 GiB memory in use. Process 3375915 has 924.00 MiB memory in use. Process 3528805 has 738.00 MiB memory in use. Including non-PyTorch memory, this process has 7.78 GiB memory in use. Of the allocated memory 7.67 GiB is allocated by PyTorch, and 21.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m y_target = extract_pitch_classes(snippet, sr=SR)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ditto_optimize_latent ensures remix_audio has len(snippet)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m remix_audio = \u001b[43mditto_optimize_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msnippet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Visualize concatenated mel\u001b[39;00m\n\u001b[32m     12\u001b[39m plot_concat_mel(snippet, remix_audio, sr=SR)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mditto_optimize_latent\u001b[39m\u001b[34m(model, vocoder, y_target, target_len_samples, sr, steps, latent_shape)\u001b[39m\n\u001b[32m     59\u001b[39m     mel_final = x_final\n\u001b[32m     61\u001b[39m mel_final = mel_final.unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# [1, 80, T]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m audio = \u001b[43mvocoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_final\u001b[49m\u001b[43m)\u001b[49m.squeeze().detach().cpu().numpy()  \u001b[38;5;66;03m# [T]\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# 🔧 Adjust output length to match original\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio) > target_len_samples:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mHiFiGANGenerator.forward\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mel):\n\u001b[32m     14\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.mel_conv(mel)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:828\u001b[39m, in \u001b[36mLeakyReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cse253-assignment2/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1902\u001b[39m, in \u001b[36mleaky_relu\u001b[39m\u001b[34m(input, negative_slope, inplace)\u001b[39m\n\u001b[32m   1900\u001b[39m     result = torch._C._nn.leaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[32m   1901\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1903\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.62 GiB of which 13.06 MiB is free. Process 4015399 has 52.00 MiB memory in use. Process 4015398 has 52.00 MiB memory in use. Process 4015428 has 52.00 MiB memory in use. Process 4015364 has 52.00 MiB memory in use. Process 4015464 has 52.00 MiB memory in use. Process 4015456 has 52.00 MiB memory in use. Process 4015383 has 52.00 MiB memory in use. Process 4015405 has 52.00 MiB memory in use. Process 4015403 has 52.00 MiB memory in use. Process 4015440 has 52.00 MiB memory in use. Process 4015373 has 52.00 MiB memory in use. Process 4015259 has 52.00 MiB memory in use. Process 4015475 has 52.00 MiB memory in use. Process 4015202 has 52.00 MiB memory in use. Process 4015471 has 52.00 MiB memory in use. Process 4015427 has 52.00 MiB memory in use. Process 4015378 has 52.00 MiB memory in use. Process 4015451 has 52.00 MiB memory in use. Process 4015460 has 52.00 MiB memory in use. Process 4015248 has 52.00 MiB memory in use. Process 4015457 has 52.00 MiB memory in use. Process 4015363 has 52.00 MiB memory in use. Process 4015397 has 52.00 MiB memory in use. Process 4015458 has 52.00 MiB memory in use. Process 4015472 has 52.00 MiB memory in use. Process 4015241 has 52.00 MiB memory in use. Process 4015237 has 52.00 MiB memory in use. Process 4015454 has 52.00 MiB memory in use. Process 4015366 has 52.00 MiB memory in use. Process 4015453 has 52.00 MiB memory in use. Process 4015465 has 52.00 MiB memory in use. Process 4015367 has 52.00 MiB memory in use. Process 4015244 has 52.00 MiB memory in use. Process 4015400 has 52.00 MiB memory in use. Process 4015239 has 52.00 MiB memory in use. Process 4015402 has 52.00 MiB memory in use. Process 4015371 has 52.00 MiB memory in use. Process 4015242 has 52.00 MiB memory in use. Process 4015201 has 52.00 MiB memory in use. Process 4015474 has 52.00 MiB memory in use. Process 4015265 has 52.00 MiB memory in use. Process 4015455 has 52.00 MiB memory in use. Process 4015382 has 52.00 MiB memory in use. Process 4015525 has 52.00 MiB memory in use. Process 4015384 has 52.00 MiB memory in use. Process 4015249 has 52.00 MiB memory in use. Process 4015240 has 52.00 MiB memory in use. Process 4015243 has 52.00 MiB memory in use. Process 4015432 has 52.00 MiB memory in use. Process 4015395 has 52.00 MiB memory in use. Process 4015329 has 52.00 MiB memory in use. Process 4015365 has 52.00 MiB memory in use. Process 4015433 has 52.00 MiB memory in use. Process 4015404 has 52.00 MiB memory in use. Process 4015206 has 52.00 MiB memory in use. Process 4015266 has 52.00 MiB memory in use. Process 4015396 has 52.00 MiB memory in use. Process 4015469 has 52.00 MiB memory in use. Process 4015459 has 52.00 MiB memory in use. Process 4015444 has 52.00 MiB memory in use. Process 4015256 has 52.00 MiB memory in use. Process 4015359 has 52.00 MiB memory in use. Process 4015213 has 52.00 MiB memory in use. Process 2307265 has 1.65 GiB memory in use. Process 3375915 has 924.00 MiB memory in use. Process 3528805 has 738.00 MiB memory in use. Including non-PyTorch memory, this process has 7.78 GiB memory in use. Of the allocated memory 7.67 GiB is allocated by PyTorch, and 21.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "combined_audio = []\n",
    "\n",
    "for i in range(2):\n",
    "    path = random.choice(wav_files)\n",
    "    snippet = extract_random_snippet(path, duration_sec=2, sr=SR)\n",
    "\n",
    "    y_target = extract_pitch_classes(snippet, sr=SR)\n",
    "    # ditto_optimize_latent ensures remix_audio has len(snippet)\n",
    "    remix_audio = ditto_optimize_latent(model, generator, y_target, len(snippet), sr=SR, steps=50)\n",
    "\n",
    "    # Visualize concatenated mel\n",
    "    plot_concat_mel(snippet, remix_audio, sr=SR)\n",
    "\n",
    "    combined_audio.append(remix_audio)\n",
    "\n",
    "# Concatenate and save the final audio\n",
    "final_audio = np.concatenate(combined_audio)\n",
    "sf.write(\"remixed_combined.wav\", final_audio, SR)\n",
    "print(\"Saved connected remix to remixed_combined.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
